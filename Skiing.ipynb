{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-24 10:19:35,311] Making new env: Skiing-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Skiing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 3\n",
      "['NOOP', 'RIGHT', 'LEFT']\n",
      "Observation space shape: (250, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALcAAAD8CAYAAAA18TUwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa10dc29e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALcAAAD8CAYAAAA18TUwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1212ec0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "print(env.get_action_meanings())\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpu:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "rl_path = 'reinforcement-learning/'\n",
    "\n",
    "if rl_path not in sys.path:\n",
    "  sys.path.append(rl_path)\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (right) and 2 (left) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 140, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa0882206d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAAD8CAYAAAAojwurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa10dc2acc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation_crop = observation[50:,10:-10,:]\n",
    "print(np.shape(observation_crop))\n",
    "plt.imshow(observation_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[250,160,3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 50, 10, 200, 140)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [200, 140, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 3 RGB frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 3], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        self.X = X\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, 3)\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 84, 84, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 84, 84, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmurzin/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 160, 3)\n",
      "[[ 0.07146844  0.          0.        ]\n",
      " [ 0.07146844  0.          0.        ]]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    print(np.shape(observation))\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 3, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 3, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[total_t])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 3, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "    import gym.wrappers\n",
    "    env = gym.wrappers.monitoring.Monitor(env, monitor_path,\n",
    "                      resume=True,\n",
    "                      video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 3, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            # This is where Double Q-Learning comes in!\n",
    "            q_values_next = q_estimator.predict(sess, next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
    "                discount_factor * q_values_next_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmurzin/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /home/nmurzin/experiments/Skiing-v0/checkpoints/model...\n",
      "\n",
      "Populating replay memory...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=0.9,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=5000000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (user)",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
